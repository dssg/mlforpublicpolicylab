## Project Work
By this week, project work should be beginning to focus more heavily on evaluation, model selection, and interpretation. 

**Due Monday, Nov 9**: Weekly project update.

## Tuesday: Model Interpretability, Part II
This week, weâ€™ll look at the other side of interpretability, with a
focus on several practical applications of local explanations: they
can help researchers debug and improve their models, build trust
among stakeholders (including a growing legal movement towards a
"right to explanation"), help those acting on model predictions
understand when they should override the model with their judgement,
and importantly help those actors decide not only on whom to intervene
but suggest what sort of intervention to take. 

Required Readings for Tuesday:
- *Why Should I Trust You? Explaining the Predictions of any Classifier* by Ribeiro, MT, Singh, S, and Guestring, C. KDD 2016. [Available Online](https://dl.acm.org/doi/abs/10.1145/2939672.2939778)
- *Explainable machine-learning predictions for the prevention of hypoxaemia during surgery* by Lundberg, SM, Nair, B, et al. Nature Biomed. Eng. 2018. [Available Online](https://www.nature.com/articles/s41551-018-0304-0.pdf)

## Wednesday: Group Check-Ins

## Thursday: Project Work

Optional Readings:
- *Model Agnostic Supervised Local Explanations* by Plumb, G, Molitor, D, and Talwalkar, AS. NIPS 2018. [Available Online](http://papers.nips.cc/paper/7518-model-agnostic-supervised-local-explanations)
- *A Unified Approach to Interpreting Model Predictions* by Lundberg, SM and Lee, S. NIPS 2017.  [Available Online](http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predicti)
- *Explainable AI for Trees* by Lundberg, SM, Erion, G, et al. arXiv preprint: arxiv/1905.04610. [Available Online](https://arxiv.org/pdf/1905.04610.pdf)
